
<!-- saved from url=(0051)https://afsweb.clarkson.edu/class/cs449/a3-s20.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252"><title>Assignments</title>
</head>
<body bgcolor="#FBFBFB">


<h2><font color="navy">Assignment 3</font></h2>
<h2><font color="gray">Boosting</font></h2>

<hr>

In this assignment, we experiment with <b>boosting</b> as a method for improving the prediction accuracy of learning algorithms.
<ol>
<li>Use scikit-learn but design your own implementation of the 
	<a href="http://rob.schapire.net/papers/SchapireSi98.pdf">AdaBoost</a> algorithm. <br>
	As a bonus, compare this with the implementation of AdaBoost in scikit-learn.

</li><li>Test the boosting algorithm on the MNIST 
	<a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html">digit</a> database.

</li><li>Choose at least two "weak" learners for your experiments. <br>
	For one of the weak learners, use the 
	<a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">decision tree classifier</a> 
	which is supported by scikit-learn. <br>
	Then, choose another weak classifier of your choice (within scikit-learn) 
	and compare this with the decision tree weak classifier.

</li><li>Provide a plot showing how the training error of the AdaBoost classifier 
	changes during the boosting process. <br>
	Determine whether boosting works effectively and which weak classifier is better.

</li></ol>

<hr>

<h3>What to submit</h3>

Submit via email your source file(s) and the plot (as PDF). Provide a brief demo your program. 
You may work in groups of three.

<hr>

<h3>Bonus</h3>

Repeat the above using a different real-world dataset (other than the MNIST digit dataset) of your choice.




</body></html>